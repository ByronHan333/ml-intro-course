{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Feature Engineering</h2></center>\n",
    "\n",
    "<center><img src=\"https://codesachin.files.wordpress.com/2016/06/the-how-and-why-of-feature-engineering-5-638.jpg\" width=\"85%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Explain why Feature Engineering (FE) is important\n",
    "- List different common FE methods\n",
    "- Describe the advantages and disadvantages of the common methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/new_oil.jpg\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/refine.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "“data is the fuel of machine learning.” This isn’t quite true: data is like the crude oil of machine learning which means it has to be refined into features — predictor variables — to be useful for training a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is Feature Engineering (FE)?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2> Representing data is the best way possible for a ML algorithm.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>The process of formulating the most appropriate features given the goal, the algorithm, and the raw data.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why is FE important?\n",
    "------\n",
    "\n",
    "<center><img src=\"images/pipeline.png\" width=\"75%\"/></center>\n",
    "\n",
    "ML algorithms assume digital, numeric inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The most valuable information in the world is often not numeric and might not even be digital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Image Source: https://www.safaribooksonline.com/library/view/feature-engineering-for/9781491953235/ch01.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the most common data types that are not ML ready?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Brian's experience with matching people to jobs...</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There is a field of study called \"Measurement Theory\" that studies the limits and proper way to measure/encode information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/succes.jpg\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Feature Engineering is the work of Data Scientists\n",
    "----\n",
    "\n",
    "<center><img src=\"images/Data_Science_VD.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " Source: https://codesachin.wordpress.com/2016/06/25/non-mathematical-feature-engineering-techniques-for-data-science/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Feature Engineering is the work of Data Scientists\n",
    "----\n",
    "\n",
    "- Takes the most time\n",
    "- Has a big impact on modeling and business value\n",
    "- Has not been automated, requires humans and domain knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://elitedatascience.com/feature-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3 common approaches to FE\n",
    "-----\n",
    "\n",
    "1. Hand crafted rules\n",
    "2. Learned models\n",
    "3. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hand crafted rules\n",
    "----\n",
    "\n",
    "A good place to start\n",
    "\n",
    "Very common\n",
    "\n",
    "Requires domain expertise\n",
    "\n",
    "Examples: The \"magic numbers\" in filtering / thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example, electrical signal processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Learned\n",
    "----\n",
    "\n",
    "Apply ML to Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Typically unsupervised (e.g., dimension reduction or clustering) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In NLP, peform topic modeling (i.e., clustering) then classification within clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Stacking\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://1.bp.blogspot.com/-S8ss-zVfpRM/V1qKcxfCvNI/AAAAAAAAD0I/8UUFyrE4MqQYYuWSxrOOvX3zRfw93nCLwCLcB/s1600/Stacking.png\" width=\"75%\"/></center>\n",
    "\n",
    "The outputs of one model become the inputs of another model\n",
    "\n",
    "`Pipeline = [Transformer, Transformer, Transformer]`\n",
    "\n",
    "More about this next session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are common FE techniques?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Handling Missing Values\n",
    "- Vectorizing\n",
    "- Filtering / Thresholding\n",
    "- Binning\n",
    "- Transforming\n",
    "    - Rescaling \n",
    "- Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Missing Values\n",
    "-----\n",
    "\n",
    "All data has missing values. Just deal with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Be very careful that the missing data is not systematic of the effect you are studying.\n",
    "\n",
    "Maybe missing data could be a feature (not a bug)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[\"Absence of evidence is not evidence of absence.\"](https://en.wikipedia.org/wiki/Evidence_of_absence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(An incomplete) list of techniques:\n",
    "\n",
    "- Drop rows (instances)\n",
    "- Drop columns (features)\n",
    "- Impute values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ways to impute values\n",
    "-----\n",
    "\n",
    "1. Go get the missing data!\n",
    "1. Sample from existing values \n",
    "1. Calculated mean of existing values\n",
    "1. Fit a model on other features to estimate missing value\n",
    "    - Regression is often used because it is multivariate mean estimation. \n",
    "    - k-NN works well.\n",
    "1. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://www.theanalysisfactor.com/seven-ways-to-make-up-data-common-methods-to-imputing-missing-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vectorizing\n",
    "-------\n",
    "\n",
    "<center><img src=\"https://ds055uzetaobb.cloudfront.net/image_optimizer/898ad3880c0fc382f91462f416bc3d126481aa36.png\" width=\"75%\"/></center>\n",
    "\n",
    "Could be any data, mostly applies to Text & Images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "word2vec (and variations) are very good approaches for text.\n",
    "\n",
    "Deep Learning works very well for images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Filtering / Thresholding\n",
    "------\n",
    "\n",
    "Always perform EDA (especially univariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remove out-of-bound values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the easiest way to handle out-of-bound values?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Prevent them!\n",
    "\n",
    "Drop down menus are the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Type-ahead is very, very nice\n",
    "\n",
    "<center><img src=\"https://runt-of-the-web.com/wordpress/wp-content/uploads/2017/12/onety-one.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Before type ahead, 20-25% of Google search queries had spelling mistakes.\n",
    "\n",
    "Type-ahead improved the business metrics more than any ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "palette = \"Dark2\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Demo: Bounding values with closures\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def make_bound_func(min_value, max_value):\n",
    "    \"Define a bound function with a certain min and max\"\n",
    "    def bound(value):\n",
    "        \"Limit value between the min and max\"\n",
    "        return min(max_value, max(value, min_value))\n",
    "    return bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bound_rbg = make_bound_func(min_value=0, max_value=255)\n",
    "\n",
    "assert bound_rbg(42) == 42\n",
    "assert bound_rbg(-1) == 0\n",
    "assert bound_rbg(256) == 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from math import inf\n",
    "\n",
    "bound_non_negative = make_bound_func(min_value=0, max_value=inf)\n",
    "\n",
    "assert bound_non_negative(42) == 42\n",
    "assert bound_non_negative(-1) == 0\n",
    "assert bound_non_negative(1_000_000) == 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Defining Outliers / Anomaly Detection\n",
    "------\n",
    "\n",
    "Again, can be done by hand or machine learned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://www.researchgate.net/profile/Mustafa_Aljumaily2/publication/321682378/figure/fig1/AS:569320483033088@1512747988945/Figure-1-anomaly-detection.png\" width=\"50%\"/></center>\n",
    "\n",
    "__2 types of outliers__:\n",
    "\n",
    "1. Generated by the same statistical process as your data (just unusual spread)\n",
    "2. Generated by a __different__ statistical process as your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Binning, aka univariate clustering\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://i.stack.imgur.com/XNIQd.jpg\" width=\"75%\"/></center>\n",
    "\n",
    "Discretize continuous values into a smaller number of \"bins\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why would you purposely lose information by downsampling your data?\n",
    " -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. It makes sense for your goal (e.g., categorize people's age by decade)\n",
    "1. Improve signal-to-noise ratio (e.g., GPS data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Fitting a model to bins reduces the impact that small fluctuates in the data has on the model, often small fluctuates are just noise. Each bin \"smooths\" out the fluctuates/noises in sections of the data.\n",
    "\n",
    "Source: https://datascience.stackexchange.com/questions/19782/what-is-the-rationale-for-discretization-of-continuous-features-and-when-should/23860#23860"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Transforming\n",
    "----\n",
    "\n",
    "- Linear: \n",
    "    - Preserves the operations of addition and scalar multiplication\n",
    "    - Example: Normalization & Standardization\n",
    "    - Generally fine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Nonlinear:\n",
    "    - Does __NOT__ preserves the operations of addition and scalar multiplication \n",
    "    - Examples: squaring a variable\n",
    "    - \"With great power, comes great responsibility\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rescaling\n",
    "------\n",
    "\n",
    "Often features are orders of magnitude different from each other.\n",
    "\n",
    "Several ML algorithms are sensitive to feature scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://stats.stackexchange.com/questions/244507/what-algorithms-need-feature-scaling-beside-from-svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Which specific algorithms are sensitive to feature scaling?\n",
    "\n",
    "What is common among them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "k-NN, SVM, and neural networks are sensitive to feature transformations.\n",
    "\n",
    "They exploit distances or similarities between data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Naive Bayes (a graphical model) and  Tree-based models learn features independently.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What's the difference between Normalization and Standardization?\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Normalization__ \n",
    "-----\n",
    "\n",
    "Rescales the values into a range of [0,1].  \n",
    "\n",
    "$$ X_{changed} = \\frac{X - X_{min}}{X_{max}-X_{min}} $$ \n",
    "\n",
    "Useful where all values need to be on the same positive scale. \n",
    "\n",
    "However, the outliers from the data set are compressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Standardization__\n",
    "-------\n",
    "\n",
    "Rescales data to have a mean ($\\mu$) of 0 and standard deviation ($\\sigma$) of 1 (unit variance).\n",
    "\n",
    "$$ X_{changed} = \\frac{X - \\mu}{\\sigma} $$        \n",
    "\n",
    "Retains outlier values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "\n",
    "Source: https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Feature Selection: What is one way to remove uninformative features?\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2> L1 regularization</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://i.stack.imgur.com/7YKyum.png\" width=\"55%\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"https://i.stack.imgur.com/59fltm.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " Source: https://stats.stackexchange.com/questions/74542/why-does-the-lasso-provide-variable-selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "scikit-learn's `feature_selection` module\n",
    "-----\n",
    "\n",
    "Let's take a look [https://scikit-learn.org/stable/modules/feature_selection.html](https://scikit-learn.org/stable/modules/feature_selection.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DL Does Automatic Feature Learning\n",
    "------\n",
    "\n",
    "<center><img src=\"http://adilmoujahid.com/images/traditional-ml-deep-learning-2.png\" width=\"1500\"/></center>\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "------\n",
    "\n",
    "- Feature Engineering (FE) might have bigger impact than algorithm selection and model tuning\n",
    "- FE is impactive when working in new domains, especially ones that are not digital native.\n",
    "- The goal of FE is change the raw data to best possible feature for a ML algorithm.\n",
    "- It is both an art (heuristic based) and a science (systematic and use ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
