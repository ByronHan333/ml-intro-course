{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Regularization\n",
    "-----\n",
    "\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/354px-Regularization.svg.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define regularization in your own words\n",
    "- Explain the connection between regularization and norms\n",
    "- Explain the differences between L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In general, what is regularization?\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regularization is the process of introducing additional information in order to prevent overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regularization discourages complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://miro.medium.com/max/1020/1*EBHQqNESUCne2mGSxCzkTg.jpeg\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://pics.me.me/everything-should-be-made-as-simple-as-possible-but-not-20308658.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Applied ML Regularization</h2></center>\n",
    "\n",
    "<center><img src=\"https://thepropertyexperts.com/wp-content/uploads/2014/09/il_fullxfull.296436703.jpg\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are all the ways we can regularize?\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. More data\n",
    "1. Cleaner data\n",
    "1. Early stopping\n",
    "1. Pruning\n",
    "1. Dropout\n",
    "1. Resampling the data\n",
    "1. Add noise to the data (e.g., data augmentation)\n",
    "1. Adding a constraint to the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Regularization in other words â€¦\n",
    "-----\n",
    "\n",
    "When there is insufficient data, regularization stabilizes our models.\n",
    "\n",
    "What does that sound like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Reduced variance__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One Way to Regularize: Add a constraint to the loss function\n",
    "-------\n",
    "\n",
    "Regularized Loss = Loss Function + Constraint\n",
    "\n",
    "Penalize large coefficients\n",
    "\n",
    "Thus reducing the complexity of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the typical loss function for linear regression?\n",
    "------\n",
    "\n",
    "Write it down (30s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Linear Regression Loss Function\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/loss.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Linear Regression Loss Function + L1 Regularization\n",
    "------\n",
    "\n",
    "<center><img src=\"images/l1.png\" width=\"75%\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " <center><img src=\"images/l1_stuff.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://rasbt.github.io/mlxtend/user_guide/general_concepts/regularization-linear/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optimitization 101: 1 dimension\n",
    "-------\n",
    "\n",
    "<center><img src=\"https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optimitization 101: 2 dimension\n",
    "-------\n",
    "\n",
    "<center><img src=\"images/2d.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://rasbt.github.io/mlxtend/user_guide/general_concepts/regularization-linear_files/l1.png\" width=\"50%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Define each item\n",
    "\n",
    "Without regularization, our objective is to find the global cost minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "L1 Regularization\n",
    "----\n",
    "\n",
    "Shrink the weights using the absolute values of the weight coefficients (i.e., the weight vector).\n",
    "\n",
    "Penalize the model by the absolute weight coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By adding a regularization penalty, our objective becomes to minimize the cost function under the constraint that we have to stay within our \"budget\" (the gray-shaded area)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://rasbt.github.io/mlxtend/user_guide/general_concepts/regularization-linear_files/l1.png\" width=\"40%\"/></center>\n",
    "\n",
    "Why the L1 regularization induces sparsity?\n",
    "\n",
    "Sparsity means as many values will be zero as possible.\n",
    "\n",
    "Think, pair, share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The \"budget\" has \"sharp edges\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lasso in 3-d\n",
    "-----\n",
    "<center><img src=\"https://web.stanford.edu/~hastie/StatLearnSparsity/images/sls.jpg\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Any questions before moving onto L2 Regularization?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Linear Regression Loss Function\n",
    "------\n",
    "\n",
    "<center><img src=\"images/loss.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Linear Regression Loss Function + L2 Regularization\n",
    "------\n",
    "\n",
    "<center><img src=\"images/l2.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/l2_defined.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Shrink the weights by computing the Euclidean norm of the weight coefficients (the weight vector )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Relationship to 2-norm\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://rasbt.github.io/mlxtend/user_guide/general_concepts/regularization-linear_files/l2.png\" width=\"50%\"/></center>\n",
    "\n",
    "Our constraint is now a circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This will appear on a job interview\n",
    "------\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*o6H_R3Do1zpch-3MZk_fjQ.png\" width=\"55%\"/></center>\n",
    "\n",
    "Explain this to someone nearby:\n",
    "\n",
    "1. Define each element in the figure\n",
    "2. Why are w* different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What shape would our constraint be if we used âˆž-norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Vector_norm_sup.svg/144px-Vector_norm_sup.svg.png\" width=\"35%\"/></center>\n",
    "\n",
    "a square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Î»: Regularization parameter\n",
    "-----\n",
    "\n",
    "Î» parameter controls the regularization strength, aka the size of the shaded area.\n",
    "\n",
    "The larger the value of the stronger the regularization of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Î» parameter can be picked or learned</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "During the optimization process, L2 regularization will drive unimportant parameters values to what value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "close to 0, but not too zero\n",
    "\n",
    "Given the shape, it will not enforce sparisity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "How does regularization impact bias?\n",
    "\n",
    "How does regularization impact variance?\n",
    "<br>\n",
    "<br>\n",
    "Think, pair, share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regularization always increase bias.\n",
    "\n",
    "Regularization might decrease variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for personal knowledge about Brian\n",
    "-----\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center><h2>What is my dog's name?</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Î»: The Dog</h2></center>\n",
    "<center><img src=\"images/lambda.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Regularization tries to reduce overfitting by introducing external factors to a model\n",
    "- Regularization can take many forms\n",
    "- You might be asked about Linear Regression Regularization during a job interview so\n",
    "    - Be able to expliain the \"lines touching\" diagram\n",
    "    - Why L1 is a diamond and L2 is circle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/elastic.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
