{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Naive Bayes for Classification</h1></center>\n",
    "\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/seashell.png\" width=\"35%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Explain each element of the Bayes Theorem \n",
    "- Apply Bayes Theorem to classification\n",
    "- Explain when and when __not__ to apply Naive Bayes\n",
    "- Walk through a example of Naive Bayes text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is classification?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Predict a single group from a discrete set of groups.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Methods for Classification\n",
    "-----\n",
    "\n",
    "1. Hand-written Rules\n",
    "1. Statistical Learning\n",
    "1. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rules for Text Classification\n",
    "----\n",
    "\n",
    "Write a series of `if then` statements by hand\n",
    "\n",
    "Examples:\n",
    "\n",
    "- If \"Viagra\" or \"Nigerian Prince\" appear in an email, then email is spam.\n",
    "- If words such as \"suck\" and \"terrible\" appear in a movie review, then the review is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are the downsides of rules?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Time intensive - To be complete, a large set of rules would be needed.\n",
    "1. Brittle - Rules could not easily apply to new situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Statistical Learning, aka Machine Learning\n",
    "-----\n",
    "\n",
    "Learn from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Machine Learning for Classification\n",
    "------\n",
    "<center><img src=\"images/pipeline.jpg\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Machine Learning > Static Rules\n",
    "------\n",
    "<center><img src=\"images/google.jpg\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Different Models of The World\n",
    "------\n",
    "<center><img src=\"images/bayesian_evol.png\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/bayesian_evol.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- θ is a model of the world. Just use model, ingore any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- X is data we observe. Just the data, ingore any model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/bayesian_evol.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [X | θ] Given a model, which is data is most likely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [X, θ] What is the joint occurrence of the data and the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [θ | X] Given the data, which model is most likely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is Bayes Rule?\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/Thomas_Bayes.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*l0MccMHzSjtpJ_mGaItVfA.jpeg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why do we want to use Bayes Rule as our ML algorithm?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://static1.squarespace.com/static/591e58f72994cab66b93f891/t/591f89a39c03e001d1e3b17e/1495241602853/bayes-rule-e1350930203949.png\" width=\"85%\"/></center>\n",
    "\n",
    "Allows us to evaluate our hypothesis, given our prior beliefs and new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayes Theorem for Classification\n",
    "-----\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Likelihood\n",
    "------\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"70%\"/></center>\n",
    "\n",
    "How likely we are to observe the data point given a specific label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Class Prior\n",
    "------\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"70%\"/></center>\n",
    "\n",
    "How likely we are to observe each of the label, ignoring the data.\n",
    "\n",
    "Also called base-rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Predictor Prior Probability\n",
    "------\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"70%\"/></center>\n",
    "\n",
    "How likely we are to observe any given data point $p(doc)$ \n",
    "\n",
    "In the case of text classification, probability of observing an individual document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Posterior Probability\n",
    "------\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"70%\"/></center>\n",
    "\n",
    "Predict probability of a label given a new observation.\n",
    "\n",
    "The goal of machine learning classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes for Text Classification\n",
    "------\n",
    "\n",
    "  $$p(\\text{class }| \\text{ doc}) = \\frac{p(\\text{doc } | \\text{ class}) \\times p(\\text{class})}{p(\\text{doc})}$$\n",
    "  \n",
    "\n",
    "- $p(\\text{class }| \\text{ doc})$: observing a particular class given a document (**Posterior**)\n",
    "- $p(\\text{doc } | \\text{ class})$: observing a document given a particular class (**Likelihood**)\n",
    "- $p(\\text{class})$: observing each of the classes (**Prior**)\n",
    "- $p(\\text{doc})$: observing an individual document (**Baserate**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dropping p(doc) from the equation\n",
    "-----\n",
    "\n",
    "Since all calculated probabilities have $p(\\text{doc})$ as their denominator, we can eliminate the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Prediction the class for a new document\n",
    "-------\n",
    "\n",
    "$ P(\\text{class } |\\text{ doc}) = P(\\text{class }) •  \\prod_{i=1}^n P(word_i | \\text{class })$\n",
    "\n",
    "$... = P(\\text{class}) \\times (P(word_1 \\text{ | class}) \\cdot P(word_2 \\text{ | class}) \\cdot ... \\cdot P(word_n \\text{ | class}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://medium.com/@theflyingmantis/text-classification-in-nlp-naive-bayes-a606bf419f8c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Being Naive: Bayes' Greatest Strength & Greatest Weakness\n",
    "------\n",
    "\n",
    "<center><img src=\"images/naive.jpeg\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive: Each word conditionally independent for each label\n",
    "-------\n",
    "\n",
    "<center><img src=\"images/bayes_rule.png\" width=\"70%\"/></center>\n",
    "\n",
    "Each word can independently predict multiple labels.\n",
    "\n",
    "For the word \"Viagra\", there could be a high probability of `spam` but could also have a high probability of `ham` (not spam). Let's say you worked as a pharmaceutical salesperson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes works very well in practice\n",
    "----\n",
    "\n",
    "<center><img src=\"http://www.azquotes.com/picture-quotes/quote-well-it-may-be-all-right-in-practice-but-it-will-never-work-in-theory-warren-buffett-86-75-88.jpg\" width=\"75%\"/></center>\n",
    "\n",
    "__Despite feature independence assumption__, NB is very common in real world applications: Email filtering, Fraud detection, Medical diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In text classification features are critical\n",
    "-------\n",
    "<center><img src=\"images/supervised_learning.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Discussion\n",
    "------\n",
    "\n",
    "What features might you want to use for text classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Data:\n",
    "    - Individual words\n",
    "    - Phrases\n",
    "    - Punctuation\n",
    "- Metadata:\n",
    "    - Length of document\n",
    "    - Language\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What about Deep Learning for Classification?\n",
    "-----\n",
    "\n",
    "Deep Learning is state-of-the-art (STOTA) because it can automatically learn features and the classifier.\n",
    "\n",
    "But sometimes, you don't have enough data or time.\n",
    "\n",
    "Naive Bayes is fast and works with very few examples. It is \"good enough\" for many uses cases and a very powerful baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Peter Norvig on Naive Bayes at Google\n",
    "-----\n",
    "\n",
    "> And it was fun looking at the comments, because you’d see things like ‘well, I’m throwing in this naive Bayes now, but I’m gonna come back and fix it it up and come up with something better later. And the comment would be from several years ago\n",
    "\n",
    "> … when you have enough data, sometimes, you don’t have to be too clever about coming up with the best algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Why is Naive Bayes \"embarrassingly parallelizable\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each feature weight can be learned separately for each class.\n",
    "\n",
    "Thus can be learned at the same time on a separate computer core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://alitarhini.wordpress.com/2011/03/02/parallel-naive-bayesian-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussian Naive Bayes: 1 dimension\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Yune_Lee/publication/255695722/figure/fig1/AS:297967207632900@1448052327024/Illustration-of-how-a-Gaussian-Naive-Bayes-GNB-classifier-works-For-each-data-point.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussian Naive Bayes: 2 dimensions\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/two_d.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussian Naive Bayes: 2 dimensions\n",
    "-----\n",
    "<center><img src=\"images/two_d_bound.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Assumes sigma hyperparamter is shard across classes.\n",
    "\n",
    "The boundary can be nonlinear is sigma hyperparameter is different for different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When does Naive Bayes NOT work, aka make useful predictions?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/anti_bayes.png\" width=\"67%\"/></center>\n",
    "\n",
    "These features are __not__ independent.\n",
    "\n",
    "Modeling correlation between features is needed to separate classes.\n",
    "\n",
    "These groups are separable but a NB classifier will perform at chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.youtube.com/watch?v=feBKiAdhYkc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further Study\n",
    "------\n",
    "\n",
    "- [Guassian Naive Bayes formula](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes)\n",
    "- Modeling a combination of discrete and real-valued features\n",
    "- Non-guassian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary, Part I\n",
    "------\n",
    "\n",
    "- Bayes Rule is one of the most important theorems in Statistics.\n",
    "- You should be apply to write Bayes Rule from memory and able to Bayes Rule to novel situations.\n",
    "- Naive Bayes (NB) is among the simplest (and most effective) machine learning classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary, Part II\n",
    "------\n",
    " \n",
    "- Discrete NB works well for text classification.\n",
    "- However, NB does not work when features correlation are needed to separate classes.\n",
    "- Gaussian Naive Bayes is a logical extension for continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"I like my data large, my algorithms simple, and my labels weak.\"\n",
    "— Andrej Karpathy"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
