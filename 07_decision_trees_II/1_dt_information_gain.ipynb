{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Decision Trees</h2></center>\n",
    "\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/tree.png\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Calculate how Information Gain can make decision tree (DT) splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>A Decision Tree for Titantic Dataset</h2></center>\n",
    "<center><img src=\"https://bigwhalelearning.files.wordpress.com/2014/11/titanic_heuristic.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Student Activity: Think, Pair, Share\n",
    "-------\n",
    "\n",
    "The root of a decision tree is located  \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_.\n",
    "\n",
    "DT node does what? What kind of split? On what?\n",
    "\n",
    "Is the root a node?\n",
    "\n",
    "Each DT leaf does what?\n",
    "\n",
    "What does it mean that DT does divide-and-conquer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A decision tree root is located at the top, thus it is the first split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Node makes a binary split based a specific value of a specific attribute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes - The root is a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A leaf classifies __all__ members in the defined feature space as belonging to a __single__ class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Divide-and-conquer \n",
    "-----\n",
    "\n",
    "Split the data into subsets, which are then split repeatedly into even smaller subsets, …\n",
    "\n",
    "The process stops when the algorithm determines the data within the subsets are sufficiently homogeneous (or another stopping criterion has been met)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://hub.packtpub.com/divide-and-conquer-classification-using-decision-trees-and-rules/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's predict movie success\n",
    "-----\n",
    "Inputs: \n",
    "\n",
    "- Number of A-list celebrities\n",
    "- Size of estimated budget\n",
    "\n",
    "Outputs:\n",
    "1. Critical success\n",
    "1. Mainstream Hit\n",
    "1. Box Office Bust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where should our first split be?\n",
    "------\n",
    "<center><img src=\"https://www.packtpub.com/sites/default/files/Article-Images/B03905_05_02.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://www.packtpub.com/sites/default/files/Article-Images/B03905_05_03.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where should our second split be?\n",
    "-----\n",
    "<center><img src=\"https://www.packtpub.com/sites/default/files/Article-Images/B03905_05_03.png\" width=\"65%\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://www.packtpub.com/sites/default/files/Article-Images/B03905_05_04.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Can DT bounds have a slope?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No - It is sometimes to slow to train a DT. It would be intricately slow to fit if a slope had to be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Can DT bounds be nonlinear?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No - It is sometimes to slow to train a DT. It would be incredibly intricately slow to fit if a nonlinear bound had to be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://www.packtpub.com/sites/default/files/Article-Images/B03905_05_05.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do we choose one split over another?\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Information Gain__\n",
    "\n",
    "aka, KL Divergence or Relative Gain or Relative Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Find the attribute that returns the highest information gain (i.e., the most homogeneous branches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Let's Work Through An Example</h2></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://www3.nd.edu/~rjohns15/cse40647.sp14/www/content/lectures/23%20-%20Decision%20Trees%202.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Are we going to have MSDS Field Day?\n",
    "-------\n",
    "\n",
    "<center><img src=\"images/data.png\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the Entropy of the outcome variable `Play`?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/h.png\" width=\"58%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's calculate the Information Gain for splitting on Humidity\n",
    "-------\n",
    "<center><img src=\"images/data.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h3>Information Gain = Entropy(parent) - weighted average of entropy(children)</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/infogain.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/hum.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we did split on Humidity, how much entropy would there be in each child? \n",
    "-------\n",
    "<center><img src=\"images/gainer.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/total.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What do we do next?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculate IG for each feature\n",
    "------\n",
    "\n",
    "Information gain for each feature:\n",
    "\n",
    "- Outlook = 0.247\n",
    "- Temperature = 0.029\n",
    "- Humidity = 0.152\n",
    "- Windy = 0.048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which one do we choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Always choice the largest IG\n",
    "-------\n",
    "\n",
    "In this case, it is `Outlook`\n",
    "\n",
    "DT are greedy - Find & choose the current best value.\n",
    "\n",
    "Greedy algorithms can have local minimums. There could be complex / future choice dependent rules that could perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the next step?\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Do it again for each subsection of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is that process called in Computer Science terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Recursion!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2><a href=\"https://www.google.com/search?q=recursion\">Let's google recursion</a></h2></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Decision Trees learn top-down, greedily, recursively.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>The goal of DT is divide the dataset into pure regions</h2></center>\n",
    "\n",
    "<center><img src=\"https://shapeofdata.files.wordpress.com/2013/07/dtree2.png\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How would we split for multi-nominal features?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Consider all possible pairs of splits. For k classes there are 2<sup>k-1</sup>-1 splits, which is computationally prohibitive if k is a large number.\n",
    "\n",
    "1. If there are ordered classes, then we do version of binary search. This reduces the search to k-1 possible splits for k classes.  \n",
    "\n",
    "1. If there are have many ks, a One-Versus-All-The-Rest reduces the search to linear for number of k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: \n",
    "\n",
    "- https://www.displayr.com/how-is-splitting-decided-for-decision-trees/\n",
    "- https://www.mathworks.com/help/stats/splitting-categorical-predictors-for-multiclass-classification.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How would we split for continuous features?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need a threshold split (i.e., x ≤ 42)\n",
    "\n",
    "One option is too sort the data and perform binary search for optional split value.\n",
    "\n",
    "This could be computational intractable because it requires sorting O(nlogn) then binary search O(logn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: \n",
    "\n",
    "- https://www.coursera.org/lecture/ml-classification/threshold-splits-for-continuous-inputs-tn6M9\n",
    "- https://www.coursera.org/lecture/ml-classification/optional-picking-the-best-threshold-to-split-on-sKrGp\n",
    "- https://www.microsoft.com/en-us/research/publication/efficient-determination-dynamic-split-points-decision-tree/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fdmax%2Fpublications%2Fsplits.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Methods for Feature Selection, <br>aka measure purity of a feature space\n",
    "------\n",
    "\n",
    "- Classification Error\n",
    "- Information Gain\n",
    "- Gini Index \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Other Splitting Methods\n",
    "-----\n",
    "\n",
    "- Gain ratio\n",
    "- Twoing criteria\n",
    "- Distance Measure\n",
    "- Likelihood-Ratio Chi–Squared Statistics\n",
    "- DKM Criterion\n",
    "- Normalized Impurity Based Criteria\n",
    "- Binary Criteria\n",
    "- Orthogonal (ORT) Criterion\n",
    "- Kolmogorov–Smirnov Criterion\n",
    "- AUC–Splitting Criteria\n",
    "\n",
    "Source: http://www.ise.bgu.ac.il/faculty/liorr/hbchap9.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Classification Error \n",
    "------\n",
    "\n",
    "$$ Classification Error = 1 - maxp_k$$\n",
    "\n",
    "where $𝑝_𝑘$ denotes the proportion of instances belonging to class\n",
    "𝑘 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Classification Error Splitting\n",
    "------\n",
    "\n",
    "<center><img src=\"images/c_e_split.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information Gain\n",
    "------\n",
    "\n",
    "<center><img src=\"https://www.normshield.com/wp-content/uploads/2017/05/formula1.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gini Index\n",
    "------\n",
    "\n",
    "A measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Computed by summing the probability $p_i$ of an item with label $i$ being chosen times the probability$\\sum_{i≠i}p_k = 1-p_i$ of a mistake in categorizing that item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gini Index  Formula\n",
    "------\n",
    "\n",
    "<center><img src=\"images/g.png\" width=\"45%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Gini INdex: Worked Example](http://dni-institute.in/blogs/gini-index-work-out-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gini Index Properties\n",
    "------\n",
    "\n",
    "<center><img src=\"images/g_prop.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparing Splitting Criteria\n",
    "------\n",
    "\n",
    "<center><img src=\"images/compare.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: Machine Learning: A Probabilistic Perspective by Murphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Limitations of Feature Selection Measures\n",
    "-----\n",
    "\n",
    "- Classification Error\n",
    "    - Bad idea (remember the problems with accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Information Gain\n",
    "    - Favors many valued features (e.g., multinomial or continuous). __Why?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Information Gain is Bias towards many valued features (e.g., multinomial or continuous) Why?\n",
    "\n",
    "All things being equal, entropy is higher in those cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gini Index\n",
    "    - Favors equal-sized partitions with purity.\n",
    "    - Has difficulties when the number of classes is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------ \n",
    "\n",
    "Why do we only make binary splits DT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Consistency - All trees look and can be interpreted the same, including across categorical and continuous variables.\n",
    "\n",
    "1. Any message can be encoded in binary (as proven by Information Theory)\n",
    "\n",
    "1. Avoid data fragmentation. Don't break the data into small subsets too quickly. \n",
    "\n",
    "1. Optimized for \"classical\" computers. DT might be different on quantum computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Decision trees split on the attribute that has the largest Information Gain\n",
    "- Information Gain = Entropy(parent) - weighted average of entropy(children)\n",
    "- DT learn top-down, greedy, recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
