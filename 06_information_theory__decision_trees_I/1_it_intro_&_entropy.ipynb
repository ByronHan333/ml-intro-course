{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://thespectrumofriemannium.files.wordpress.com/2012/02/it.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Explain the general principles of Information Theory\n",
    "- Define Entropy in the context of Information Theory\n",
    "- Explain how Entropy can be used in Statistics / Machine Learning\n",
    "- Calculate the Entropy of common statistical events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information Theory (IT)\n",
    "------\n",
    "<center><img src=\"https://www.researchgate.net/profile/Fareed_Al-Hindawi/publication/313928362/figure/fig2/AS:493421879140353@1494652351277/Figure-no-2-Shannon-Weaver-Information-Theory-1949-This-diagram-refers-to-the.png\" width=\"75%\"/></center>\n",
    "The goal of IT is to define the fundamental limits on signal processing and communication, such as data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information Theory Greatest Hits\n",
    "------\n",
    "\n",
    "- Modern computers\n",
    "- Internet\n",
    "- Telecommunications systems, including mobile phones\n",
    "- Computational linguists modeling\n",
    "- Understanding of black holes\n",
    "- Voyager missions to deep space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why do we care about IT?\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://bricaud.github.io/personal-blog/images/entropy/splitdiagram.png\" width=\"75%\"/></center>\n",
    "\n",
    "Information Theory is how decision tree actually decide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "IT History\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"http://stanstudio.com/wp-content/uploads/2012/02/Claude_Shannon_Juggling.jpg\" height=\"500\"/></center>\n",
    "\n",
    "Originally proposed by Claude E. Shannon in __1948__ in a landmark paper entitled \"A Mathematical Theory of Communication\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"Entropy\" is the key concept in information theory\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://imagecache5d.allposters.com/watermarker/62-6264-ZSG5100Z.jpg?ch=671&cw=894&type=cns\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in Physics\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://d2jmvrsizmvf4x.cloudfront.net/rvkaVrvITeYKGO3EMmeG_entropy.jpg\" height=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Entropy in Thermodynamics\n",
    "-----\n",
    "\n",
    "The second law of thermodynamics states that the total entropy of an isolated system can only increase over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br>\n",
    "<center><img src=\"http://s2.quickmeme.com/img/cd/cd9ac5d71167288007fe7d9b45dc8faa7229529028a54797cba49b55e0700963.jpg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in Information Theory\n",
    "-----\n",
    "\n",
    "Claude Shannon defined the fundamental units of __information__, the smallest possible chunk that cannot be divided any further.\n",
    "\n",
    "He called the units \"bits\". Bit is short for binary digit: 0 or 1.\n",
    "\n",
    "Groups of which can be used to encode __any__ message. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/def.png\" height=\"500\"/></center>\n",
    "\n",
    "Shannon entropy is the quantity H, a measure of the information in a message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sum up the probabilities of the all possible symbols (x) that might turn up in a message, weighted by the number of bits needed to represent the value of that symbol (x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logarithms review\n",
    "-----\n",
    "\n",
    "<br>\n",
    "<center><img src=\"http://science.larouchepac.com/gauss/ceres/InterimII/Arithmetic/Primes/Log_Exp_inverts.jpg\" width=\"400\"/></center>\n",
    "\n",
    "A logarithm is an inverse exponential function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "b<sup>x</sup> = y is equivalent to saying x = log<sub>b</sub>y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Properties of exponentials and logarithms\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"https://people.richland.edu/james/lecture/m116/logs/log2.gif\" width=\"300\"/></center>\n",
    "\n",
    "Exponential functions grow at a distressingly fast rate, as anyone who has ever tried to pay off a credit card balance understands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, logarithms, inverse exponential functions, grow very slowly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logarithms arise in any process where things are repeatedly halved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "palette = \"Dark2\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bits\n",
    "--------\n",
    "\n",
    "If we have legnth of 1, there are 2 bits: {0, 1}\n",
    "\n",
    "If we have length of 2, there are 4 bits: {00, 01, 10, 11}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bits & Logarithms\n",
    "--------\n",
    "\n",
    "How many bits do we need to represent any one of n different possibilities (one of n items or the integers from 1 to n)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The key observation is that there must be at least __n__ different bit patterns of length __w__. \n",
    "\n",
    "Since the number of different bit patterns doubles as you add each bit, we need at least __w__ bits where __2<sup>w</sup> = n__.\n",
    "\n",
    "We need w = log<sub>2</sub>n bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items|# of bits\n",
      "         2         1\n",
      "         6         3\n",
      "        52         6\n",
      "     1,000        10\n",
      "    10,000        14\n",
      "   100,000        17\n",
      " 1,000,000        20\n"
     ]
    }
   ],
   "source": [
    "Ns = [2, 6, 52, 1_000, 10_000, 100_000, 1_000_000]\n",
    "\n",
    "print(f\"{'# of items':>10}|{'# of bits':>9}\")\n",
    "for n in Ns:\n",
    "    print(f\"{n:>10,} {ceil(log(n, 2)):>9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy Formula\n",
    "-------\n",
    "\n",
    "H = -Σ p<sub>i</sub> • log<sub>2</sub>p<sub>i</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each symbol in a message, find the probability then multiple it by the log of it. \n",
    "\n",
    "Sum up all of those and take the negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The base-2 measure of entropy has sometimes been called the \"Shannon Entropy\" in Claude Shannon's honor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Base-2 logarithms \n",
    "------\n",
    "\n",
    "Base-2 logarithms create units called __bits__ (or shannons)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Natural logarithm, with base e, then our units would be __nats__. \n",
    "\n",
    "One nat is the amount of information gained by observing an event of probability 1/e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy Symbol\n",
    "-------\n",
    "<br>\n",
    "<center><img src=\"images/e_f.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ℍ is very ugly Unicode so I'll just use H."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are only going to use base-2 and discrete items. \n",
    "\n",
    "Information Theory extends to other bases and continuous levels of measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in Statistics\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/im_so_random.png\" height=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy is the measure of the uncertainty in a random variable.\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuitively, the entropy of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The entropy of variable X is defined as:  \n",
    "\n",
    "H(X) = -Σ p(x) • log<sub>2</sub>p(x)\n",
    "\n",
    "p(x) = Pr{X = x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy of a single coin flip\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"http://68.media.tumblr.com/bb49b60c0e17387aef2d3da24f0ef40a/tumblr_n30n5iMJgT1sa11jco1_500.gif\" height=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we have a fair coin, the probability occurring p(heads) and p(tails) are each 50%.\n",
    "\n",
    "H = -Σ(p<sub>i</sub> • log<sub>2</sub>(p<sub>i</sub>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "H = -(.5*log(.5, 2) + .5*log(.5, 2))\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once we have flipped a coin, we have gained one bit of information or reduced our uncertainty by one bit.\n",
    "\n",
    "This makes intuitive sense: 0 = Heads, 1 = Tails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy of a single \"weighted\" coin flip\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"https://izbicki.me/img/uploads/2011/11/coins-all.jpg\" height=\"500\"/></center>\n",
    "\n",
    "p(H) = .2  \n",
    "P(¬H) = 1-.2 = .8\n",
    "\n",
    "What is H? \n",
    "\n",
    "H = -Σ(pi • log2(pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "from numpy import log2\n",
    "\n",
    "H = -(.2*log2(.2) + .8*log2(.8))\n",
    "print(f\"{H:.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011\n"
     ]
    }
   ],
   "source": [
    "p_heads = .001\n",
    "H = -(p_heads*log2(p_heads) + (1-p_heads)*log2(1-p_heads))\n",
    "print(f\"{H:.2}\") # Way less entropy, aka far more predicatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "p_heads = 0\n",
    "\n",
    "# # What will this be?\n",
    "# H = -(p_heads*log2(p_heads) + (1-p_heads)*log2(1-p_heads))\n",
    "# print(f\"{H:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Binary entropy function\n",
    "------\n",
    "\n",
    "The special case of information entropy for a random variable with two outcomes:\n",
    "<br><br>\n",
    "<center><img src=\"images/bin_ent.svg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p_success = .5\n",
    "H = -p_success*log2(p_success) - (1-p_success)*log2(1-p_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bernoulli random variable entropy as a function of probability of success.\n",
    "------\n",
    "\n",
    "<center><img src=\"images/curve.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/events.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the Shannon entropy of the English alphabet*?\n",
    "-----\n",
    "\n",
    "English has 26 letters (a-z). \n",
    "\n",
    "<sub>* if each character is equally likely</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7004\n"
     ]
    }
   ],
   "source": [
    "n = 26\n",
    "H = -((1/n)*log2(1/n)*n)\n",
    "print(f\"{H:.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7004\n"
     ]
    }
   ],
   "source": [
    "print(f\"{log2(n):.5}\") # Or reduce to more simple form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "https://www.quora.com/What-is-the-27th-letter-of-the-English-alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the Shannon entropy of \"hello\" in ASCII?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "H = -Σ(p<sub>i</sub> • log<sub>2</sub>(p<sub>i</sub>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The probability of each symbol:  \n",
    "p(\"h\") = 1/5   \n",
    "p(\"e\") = 1/5   \n",
    "p(\"l\") = 1/5  \n",
    "p(\"l\") = 1/5    \n",
    "p(\"o\") =  1/5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The PMF:  \n",
    "----\n",
    "p(\"h\") = p(\"e\") =  p(\"0\") = 1/5   \n",
    "p(\"l\") = 2/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92\n"
     ]
    }
   ],
   "source": [
    "H = -((1/5)*log2(1/5) + \n",
    "      (1/5)*log2(1/5) +\n",
    "      (1/5)*log2(1/5) +\n",
    "      (2/5)*log2(2/5))\n",
    "print(f\"{H:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Shannon entropy\n",
    "-----\n",
    "\n",
    "The average minimum number of bits needed to encode a symbol from a string of symbols, based on the probability of each symbol appearing in that string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "If H = 1.92, how many bits do you need per symbol to encode \"hello\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2 bits.\n",
    "\n",
    "The entropy is 1.92 but bits are discrete / integers so we take the ceiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, Shannon Entropy is the…\n",
    "------\n",
    "\n",
    "<center><img src=\"http://www.science4all.org/wp-content/uploads/2013/03/Noisy-Communication2.png\" width=\"500\"/></center>\n",
    "\n",
    "Absolute limit on the best possible lossless encoding or compression of any communication assuming that the communication may be represented as a sequence of iid random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The entropy of a source that emits a sequence of N symbols that are independent and identically distributed (iid) is N·H bits (per message of N symbols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in other words\n",
    "------\n",
    "\n",
    "Entropy is the minimum descriptive complexity of a random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/self.svg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, I(x) is the self-information, which is the entropy contribution of an individual message, and 𝔼X is the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A property of entropy is that it is maximized when all the messages in the message space are equiprobable p(x) = 1/n; i.e., most unpredictable, in which case H(X) = log n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in yet other words\n",
    "------\n",
    "\n",
    "Entropy is the lower bound on the average number of yes/no questions to guess the state of a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember: Binary search (yes/no) to guess where the needle is in a sorted haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy (one last time)\n",
    "-----\n",
    "\n",
    "Entropy is sometimes called a measure of surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Information theory is concerned with representing data in a compact fashion.\n",
    "- Entropy is the measure of the uncertainty in a random variable.\n",
    "- The entropy of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known.\n",
    "- Entropy is calculated as: H = -Σ(p<sub>i</sub> • log<sub>2</sub>(p<sub>i</sub>))\n",
    "- That formula allows us to common across domains and random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy is one of many measures of uncertainty!\n",
    "\n",
    "[Learn more here](https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/active/slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let X be a discrete random variable with alphabet 𝓧 and probability mass function p(x)\n",
    "\n",
    "p(x) = Pr{X = x}, x ∈ 𝓧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The alphabet 𝓧 is all possible outcomes (I know that is yet another \"x\" but we need it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy is the expected “compressibility” of a single bit under the best encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Using IT to calculate how many tweets could there be?](https://what-if.xkcd.com/34/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Differential entropy\n",
    "-----\n",
    "\n",
    "Differential entropy (also referred to as continuous entropy) is a concept in information theory that began as an attempt by Shannon to extend the idea of (Shannon) entropy, a measure of average surprisal of a random variable, to continuous probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
