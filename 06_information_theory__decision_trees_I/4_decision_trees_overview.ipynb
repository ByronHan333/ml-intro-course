{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Decision Tree Overview</h2></center>\n",
    "<center><img src=\"images/unicorn_shirt.jpg\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define a Decision Tree in your words\n",
    "- Be able to explain how Decision Tree make splits based on Information Gain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is a Decision Tree?\n",
    "------\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Laura_Martignon/publication/280443882/figure/fig2/AS:284458268807172@1444831544419/Simple-decision-trees-used-in-the-medical-domain.png\" width=\"75%\"/></center>\n",
    "\n",
    "A series of binary splits that define a path which leads to an outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decision Tree Creation: Hand Written or Learned\n",
    "-----\n",
    "\n",
    "Trying to capture the process of what an expert does. Thus when hand written, they are sometimes called \"expert systems\". Common in medical fields or flying airplanes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We are Data Scientist so we are going to learn our Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why are Decision Trees so popular?\n",
    "-----\n",
    "\n",
    "A type of supervised Machine Learning algorithm.\n",
    "\n",
    "Both input and output variables can be either categorical or continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus Classification or Regression with any kind of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why are Decision Trees so popular?\n",
    "-----\n",
    "\n",
    "1. Accurate\n",
    "1. Flexible \n",
    "1. Interpretable\n",
    "1. Extensible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>DT Toy Example</h2></center>\n",
    "\n",
    "<center><img src=\"images/toy.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Decision Tree learns to segment the feature space.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>Decision Tree learns a series of splitting rules.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/labels.png\" width=\"45%\"/></center>\n",
    "\n",
    "- Root: The first split\n",
    "- Node: Where a feature is split\n",
    "- Leaf: After all splits, the final prediction. Also called terminal node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to grow a Decision Tree?\n",
    "------\n",
    "\n",
    "<center><img src=\"https://www.jeremyjordan.me/content/images/2017/03/Screen-Shot-2017-03-11-at-10.15.37-PM.png\" width=\"75%\"/></center>\n",
    "\n",
    "1. Divide and conquer - split the dataset into smaller problem\n",
    "\n",
    "1. Top-down - From a single split\n",
    "\n",
    "1. Greedily - Choose the split that has results in the most pure feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When do we stop growing a  Decision Tree?\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. All instances are in same class (simplest and most often used)\n",
    "1. The number of instances is less than a specified minimum\n",
    "1. The tree is deeper than some minimum\n",
    "1. The improvement of class impurity is less than a specified minimum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DT building in other words\n",
    "--------\n",
    "\n",
    "Recursively partition until all the data items belong to the same class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do we learn to make the particular split?\n",
    "-----\n",
    "\n",
    "We need some measure of how badly a region currently classifies data and how much it can improve if its split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want less randomness in a split.\n",
    "\n",
    "What does that should like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Less entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We measure that by Information Gain (aka, KL Divergence)\n",
    "-----\n",
    "\n",
    "The information gain is the decrease in entropy after a dataset is split on an attribute. \n",
    "\n",
    "Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: http://www.saedsayad.com/decision_tree.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/gain.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Steps in DT\n",
    "------\n",
    "\n",
    "1. Start with a single leaf node containing all data\n",
    "1. Search for a decision rule for a single value on a single dimensions that maximally reduces the entropy\n",
    "1. Stop based on criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Steps in DT\n",
    "------\n",
    "\n",
    "1. Calculate the entropy of the dataset\n",
    "1. For every feature, calculate the entropy for all values\n",
    "1. Pick feature and value that has the highest Information Gain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Decision Trees (DT) are among best the type of ML algorithms because\n",
    "    - Flexible: Discrete or continuous features or targets\n",
    "    - Performant: Great evaluation metric performance \n",
    "- DT decide to split based on Information Gain\n",
    "- We'll be spend most of the rest of term on them (and some of the next ML course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information-based Loss Function\n",
    "-----\n",
    "\n",
    "Some methods looked at uncertainty at a __single point__. \n",
    "\n",
    "What do we know about single point estimations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Single points estimations are always wrong!\n",
    "\n",
    "There must be a __better way__! Estimate distributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ï¿¼What is Information Gain?\n",
    "------\n",
    "\n",
    "how much is the uncertainty of Y reduced, once X is known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model information gained\n",
    "------\n",
    "\n",
    "Maximize reduction in model entropy between posterior and prior (reduce number of bits required to describe distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model information gained\n",
    "------\n",
    "\n",
    "Maximize KL divergence between posterior and prior\n",
    "<center><img src=\"images/kl.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "Label each of the following:\n",
    "    Draw the diagram that connects entropy, joint entropy, conditional entropy and mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/con_entropy.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- cross entropy\n",
    "- soft max classifers\n",
    "- Categorical Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
