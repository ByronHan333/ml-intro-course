{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/kl_4.jpg\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Explain K-L Divergence what is\n",
    "- Apply K-L Divergence to solve Data Science problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kullback–Leibler (K-L) Divergence is Relative Entropy\n",
    "------\n",
    "\n",
    "<center><img src=\"http://autoredistrict.org/images/entropies.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: http://autoredistrict.org/information_measures.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Relative entropy__ (yet another kind of entropy) \n",
    "------\n",
    "The \"distance\" between two probability mass functions p(x) and q(x) is defined as:\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*kxhjmHU5fT5M2_w__D4MMg.png\" width=\"60%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kullback–Leibler (K-L) Divergence: A way of comparing two probability distributions\n",
    "------\n",
    "\n",
    "<center><img src=\"images/kl_4.jpg\" width=\"70%\"/></center>\n",
    "\n",
    "There is a baseline probability distribution p(X).\n",
    "\n",
    "And a comparsion probability distribution q(X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "How would compare the just the __expected values__ of two probability distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Two sample t-test or permutation test__\n",
    "\n",
    "KL Divergence is far more useful because it compares the entire distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence as a compression algorithm\n",
    "-----\n",
    "\n",
    "The number of __average additional bits per data point__ necessary for compression …\n",
    "\n",
    "if we compress data in a manner that __assumes__ q(X) is the distribution underlying some data.\n",
    "\n",
    "But in reality, p(X) is the \"ground-truth\" distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence in other words\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://i.stack.imgur.com/Io7JY.jpg\" width=\"40%\"/></center>\n",
    "\n",
    "A measure of the information lost when Q is used to approximate P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence Formula\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"images/kl_from.svg\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence Formula Redux\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/kl_formula2.png\" width=\"60%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"images/cross_ent.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "K-L Divergence as a Comparison Method\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "P probability distribution (baseline) is empirical data\n",
    "\n",
    "Q probability distribution (comparsion) is a model, say a distrbution with a estimated (see next session for more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "K-L Divergence lets you estimate the probability of getting an empirical frequency distribution close from a large number of independent random variables with distribution q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/discrete.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "K-L divergence for Parameter Estimation\n",
    "-------\n",
    "\n",
    "For ŷ to y is simply the *difference* between cross entropy and entropy:\n",
    "\n",
    "$$ \\mbox{KL}(y~||~\\hat{y})\n",
    "= \\sum_i y_i \\log \\frac{1}{\\hat{y}_i} - \\sum_i y_i \\log \\frac{1}{y_i}\n",
    "= \\sum_i y_i \\log \\frac{y_i}{\\hat{y}_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "K-L divergence for Parameter Estimation\n",
    "-------\n",
    "\n",
    "It measures the number of *extra* bits we'll need on average if we encode symbols from y according to ŷ.\n",
    "\n",
    "You can think of it as a bit tax for encoding symbols from y with an inappropriate distribution ŷ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "__Note__: that minimizing cross entropy is the same as minimizing the KL divergence from ŷ to y. (They're equivalent up to an additive constant, the entropy of y, which doesn't depend on ŷ.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/cont.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence Example: Comparing 2 distributions\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/guass.png\" width=\"60%\"/></center>\n",
    "\n",
    "Where is K-L Divergence zero? Why?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where is K-L Divergence highest? Why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why is sometimes positve and sometimes negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence Example: Comparing many distributions\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/model.png\" width=\"80%\"/></center>\n",
    "\n",
    "Black is baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "ML as a minimization of KL divergence.\n",
    "------\n",
    "\n",
    "<center><img src=\"images/parameter.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "When would K-L Divergence be 0 between two distributions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "iff q = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence is non-symmetric\n",
    "------\n",
    "\n",
    "A non-symmetric measure of the difference between two probability distributions P and Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, comparing a theory to evidence is __not__ the same as comparing evidence to theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Therefore, __K-L Divergence is not a distance metric__!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kullback–Leibler (K-L) Divergence Aliases\n",
    "------\n",
    "\n",
    "- Information divergence\n",
    "- Information gain (used in decision tree terminology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Applications of K-L Divergence\n",
    "-----\n",
    "\n",
    "- [t-SNE](http://jotterbach.github.io/2016/05/23/TSNE/) and [t-SNE caveats](https://distill.pub/2016/misread-tsne/)\n",
    "- Bayesian hypothesis testing\n",
    "- Language model performance as measured in perplexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Kullback-Leibler divergence is a mapping between two probability functions.\n",
    "- KL divergence can be used for estimating the fit between two different probability distribution models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The K-L Divergence is always non-negative,\n",
    "----\n",
    "<br>\n",
    "<center><img src=\"images/zero.svg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If P = Q everywhere, then D<sub>KL</sub>(P‖Q) = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Otherwise there is an expected number of extra bits that most be used to identify that a value x from X coressponds to Q probability distribution than P probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://www.yumpu.com/en/image/facebook/11903915.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence & t-SNE\n",
    "-----\n",
    "\n",
    "t-SNE uses K-L Divergence to compress representations for easier visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " Notice that t-SNE does not retain distances but probabilities,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[learn more here](http://www.cs.toronto.edu/~hinton/absps/tsne.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"https://pbs.twimg.com/media/CEaluHhUEAA-_4V.png\" width=\"300\"/></center>\n",
    "\n",
    "If I compress word2vec to 3 dimensions from 128 with t-SNE, can I transform back to 128 dimensions later?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__NO__\n",
    "\n",
    "K-L Divergence is asymmetric. There many higher dimensions distributions that have the same relative entropy from the lower dimensions distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "eg: difference in 2 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized mutual information\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"images/norm.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "V-Measure\n",
    "-----\n",
    "\n",
    "External entropy-based cluster evaluation measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence formula\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/kl_long.png\" height=\"500\"/></center>\n",
    "\n",
    "<center><img src=\"images/kl_3.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "K-L Divergence formula\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/kl_formula.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
