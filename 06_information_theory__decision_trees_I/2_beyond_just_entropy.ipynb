{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Write a joint and conditional probability distribution for a given dataset\n",
    "- Define and calculate the following Information Theory concepts:\n",
    "    - Joint entropy\n",
    "    - Conditional entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Probability Distributions: Joint vs Conditional\n",
    "--------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Joint probability distribution: p(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Conditional probability distribution: p(y|x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given the data:    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(x, y)     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\\-\\-\\-\\-\\-  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(1, 0)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(1, 0)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(2, 0)   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(2, 1)   \n",
    "\n",
    "Make the following tables:\n",
    "\n",
    "- __Joint probability distribution p(x,y)__ \n",
    "- __Conditional probability distribution p(y|x)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "__Joint probability distribution p(x,y)__:\n",
    "\n",
    "| __p(x,y)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | ?  | ?  |\n",
    "| x=2 | ? | ? |\n",
    "\n",
    "\n",
    "__Conditional probability distribution p(y|x)__:\n",
    "\n",
    "| __p(y pipe x)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | ? | ? |\n",
    "| x=2 | ? | ? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| __p(x,y)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | 1/2 | 0 |\n",
    "| x=2 | 1/4 | 1/4 |\n",
    "\n",
    "| __p(y pipe x)__ | y=0 | y=1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| x=1 | 1 | 0 |\n",
    "| x=2 | 1/2 | 1/2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Joint entropy\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Joint Entropy \n",
    "-----\n",
    "The joint entropy of two discrete random variables X and Y is the entropy of their pairing: (X, Y).\n",
    "\n",
    "<center><img src=\"images/joint.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparing Independent Joint Probability and  Joint Entropy\n",
    "-------\n",
    "\n",
    "If X and Y are independent:\n",
    "\n",
    "X ⊥ Y then __P(X, Y)__ = P(X)P(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our uncertainty is maximal:\n",
    "\n",
    "X ⊥ Y then __H(X, Y)__ = H(X) + H(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "__NOTE__: You multiple independent probabilities and add entropy. Taking the log makes for simpler math!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our joint uncertainty is ≥ our marginal uncertainty\n",
    "\n",
    "H(X, Y) ≥  H(X) ≥ H(Y) ≥ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general, considering events jointly reduces our uncertainty\n",
    "\n",
    "H(X, Y) ≤  H(X) + H(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conditional entropy\n",
    "-----\n",
    "\n",
    "H(Y | X) is how much uncertainty is left in Y, once X is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__or__\n",
    "\n",
    "Quantifies the amount of information needed to describe the outcome of a random variable Y given that the value of another random variable X is known. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conditional entropy formula\n",
    "-----\n",
    "\n",
    "<center>H(Y | X) = H(X, Y) - H(X)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br>\n",
    "<center><img src=\"images/conditional.svg\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information never hurts\n",
    "------\n",
    "\n",
    "H(X | Y) ≤ H(Y) \n",
    "\n",
    "Conditioning on data decreases our uncertainty<sup>*</sup>.\n",
    "\n",
    "<sub>Almost always. Or at least never increases uncertainty. On average!</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Conditional_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2 Kinds of Conditional Entropy\n",
    "------\n",
    "\n",
    "Entropy can be conditioned on\n",
    "\n",
    "1. A random variable __H(Y | X)__\n",
    "2. Random variable taking a certain value __H(Y|X=x)__\n",
    "\n",
    "Care should be taken not to confuse these two definitions of conditional entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generally the specific conditional entropy of Y given a specific value x of X is more useful:\n",
    "\n",
    "H(Y | X=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "If H(Y |X = x) = 0, then what does x tells us about Y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "x accounts for all the uncertainty of Y.\n",
    "\n",
    "If we know the value of x we know the value of Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Rosetta Stone of IT <br>(What would DS be without Venn diagrams!)\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Alejandro_Villaverde/publication/261443288/figure/fig1/AS:213423871795203@1427895624743/Graphical-representation-of-the-entropies-HX-HY-joint-entropy-HX-Y.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Η(X) - Entropy of X\n",
    "- Η(Y) - Entropy of X\n",
    "- Η(X,Y) - Joint Entropy\n",
    "- Η(X|Y) - Conditional entropy of X given Y\n",
    "- Η(Y|X) - Conditional entropy of Y given X\n",
    "- I(X;Y) - Mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Futher Study for Information Theory\n",
    "-----\n",
    "\n",
    "- Loss metric (e.g., binary cross entropy)\n",
    "\n",
    "- Pointwise Mutual Information (PMI)\n",
    "\n",
    "- Maximum Entropy Modeling / MaxEnt\n",
    "\n",
    "- Markov chains/fields for sufficient statistics\n",
    "\n",
    "- Kolmogorov complexity for minimum message length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Building from joint and conditional probability distributions, we can find the corresponding entropies.\n",
    "- Joint entropy is the entropy of their pairing.\n",
    "- Conditional entropy is the entropy of one given another.\n",
    "- This is just an overview of the landscape, applying IT successfully requires practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example Problem\n",
    "------\n",
    "\n",
    "X(n) will be Success (or 1) when n is even.  \n",
    "Y(n) will be Success (or 1) when n is prime.\n",
    "\n",
    "Let's make the outcome table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| n | 1 |   2 |  3 |  4 |  5 |  6 |  7 |  8 |\n",
    "|:-------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "| X | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 |\n",
    "| Y | 0 | 1 | 1 | 0 | 1 | 0 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Joint distribution\n",
    "------\n",
    "|__X__/__Y__ | 0 | 1 |  \n",
    "|:-------:|:------:|:------:|\n",
    "| 0 |1/8 | 3/8 |\n",
    "| 1| 3/8 | 1/8 |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.81\n"
     ]
    }
   ],
   "source": [
    "from numpy import log2\n",
    "\n",
    "H_X_Y = -((1/8)*log2(1/8)+\n",
    "          (3/8)*log2(3/8)+\n",
    "          (3/8)*log2(3/8)+\n",
    "          (1/8)*log2(1/8))\n",
    "print(f\"{H_X_Y:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutual Information\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Mutual_Information_Examples.svg\" width=\"50%\"/></center>\n",
    "\n",
    "Measures the mutual dependence between the two variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutual Information\n",
    "-----\n",
    "\n",
    "Measures the amount of information that can be obtained about one random variable by observing another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutual Information Formula\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/mutual.svg\" width=\"80%\"/></center>\n",
    "\n",
    "where SI (Specific mutual Information) is the pointwise mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The mutual information between random variables X and Y is a function of their joint probability mass function p(x,y) and marginal probability mass functions p(x) and p(y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutual Information Property\n",
    "-----\n",
    "\n",
    "I(X; Y) = H(X) - H(X|Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Knowing Y, we can save an average of I(X; Y) bits in encoding X compared to __not__ knowing Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "I(X;Y) = 0 then what do we know about X and Y...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "X and Y are independent from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mutual information is a measure of the mutual dependence between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy for Search Engine Results Page (SERP)\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://searchengineland.com/figz/wp-content/seloads/2012/10/Twitter-Search-Screenshot-.png\" width=\"30%\"/></center>\n",
    "\"Clickology\" is one part of Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Click entropy\n",
    "-----\n",
    "\n",
    "Entropy can apply to the probability of clicking.\n",
    "\n",
    "What does it mean that there is __low click entropy__ for SERP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If almost everyone clicks on that result, that query's click entropy is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Examples of __low click entropy__ for SERP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Factual queries\n",
    "- Unambiguous queries\n",
    "- Trending queries\n",
    "- High precision results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What would it mean if there was high entropy for a SERP?\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "High entropy means clicks by people are distributed uniformly across results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "__PROTIP__ If you already have good search system and still have high click entropy for some searches, personalized search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross entropy\n",
    "----\n",
    "\n",
    "\n",
    "H(p, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=tRsSi_sqXjI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross Entropy Formula\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/cross.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "mean cross entropy (MXE)\n",
    "------\n",
    "\n",
    "\n",
    "predicting the probability that an example is positive \n",
    "\n",
    "It can be proven that in this setting minimizing the cross entropy gives the maximum likelihood hypothesis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/mxe.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Softmax classifier\n",
    "-----\n",
    "\n",
    "loss function\n",
    "\n",
    "a binary Logistic Regression classifier generalized to multiple classes\n",
    "\n",
    "used to minize cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/soft.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Categorical Cross-Entropy Loss\n",
    "-----\n",
    "\n",
    "The categorical cross-entropy loss is also known as the negative log likelihood. \n",
    "\n",
    "Measures the similarity between two probability distributions, \n",
    "typically the true labels and the predicted labels. \n",
    "\n",
    "It is given by L = - sum(y * log(y_prediction)) \n",
    "where y is the probability distribution of true labels (typically a one-hot vector) \n",
    "and y_prediction is the probability distribution of the predicted labels, \n",
    "often coming from a softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross Entropy\n",
    "------\n",
    "\n",
    "between two probability distributions over the same underlying set of events \n",
    "\n",
    "the average number of bits needed to identify an event drawn from the set,\n",
    "if a coding scheme is used that is optimized for an \"unnatural\" probability distribution q, rather than the \"true\" distribution p.\n",
    "\n",
    "between two probability distributions over the same underlying set of events measures \n",
    "the average number of bits needed to identify an event drawn from the set, \n",
    "if a coding scheme is used that is optimized for an \"unnatural\" probability distribution q,\n",
    "rather than the \"true\" distribution p.\n",
    "\n",
    "The cross entropy for the distributions p and q over a given set is defined as follows:\n",
    "<center><img src=\"images/cross_def.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What is H(X)?  \n",
    "What is H(X)?  \n",
    "What is H(X)+H(Y)?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "H(X) = 1 = (-((1/2)*log2(1/2)+(1/2)*log2(1/2))  \n",
    "H(Y) = 1 = (-((1/2)*log2(1/2)+(1/2)*log2(1/2))  \n",
    "H(X)+H(Y) = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is less entropy in joint than the marginals\n",
    "\n",
    "(H(X, Y) ≈ 1.811) < (H(X) + H(Y) = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mutual information is symmetric\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"images/sym.svg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information\n",
    "-----\n",
    "\n",
    "Mutual information is the communication rate in the presence of noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"images/mutual_information.jpg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/proof.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
