{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Decision Tree Overview</h2></center>\n",
    "<center><img src=\"images/unicorn_shirt.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define a Decision Tree in your words\n",
    "- Be able to explain how Decision Tree make splits based on Information Gain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is a Decision Tree?\n",
    "------\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Laura_Martignon/publication/280443882/figure/fig2/AS:284458268807172@1444831544419/Simple-decision-trees-used-in-the-medical-domain.png\" width=\"75%\"/></center>\n",
    "\n",
    "A series of binary splits that define a path that leads to an outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decision Tree Creation: Hand Written or Learned\n",
    "-----\n",
    "\n",
    "When hand written, they are sometimes called \"expert systems\". Trying to capture the process of what an expert does. Examples are medical or piloting air craft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We are Data Scientist so we are going to learn our Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why are Decision Trees so popular?\n",
    "-----\n",
    "\n",
    "A type of supervied Machine Learning algorithm\n",
    "\n",
    "Both input and output variables can be either categorical or continuous\n",
    "\n",
    "Classfication & Regression with any kind of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We are Data Scientist so we are going to learn our Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decision Tree Terms:\n",
    "-----\n",
    "\n",
    "- Root\n",
    "- Node\n",
    "- Leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DT Toy Example\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/toy.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do we make a leaf?\n",
    "-----\n",
    "\n",
    "Splitting objective with entropy and information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: http://www.saedsayad.com/decision_tree.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "￼What is Information Gain?\n",
    "-----\n",
    "\n",
    "\n",
    "One has to transmit Y ____________\n",
    "\n",
    "How many fraction of bits on average would it save if both ends of the line would know X?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information gain for decision trees\n",
    "------\n",
    "\n",
    "Based on the concept of entropy from information theory.\n",
    "\n",
    "Information gain in decision trees\n",
    "\n",
    "a synonym for Kullback–Leibler divergence, the amount information two distributions share\n",
    "\n",
    "1. Calculate entropy of the target\n",
    "\n",
    "2. The dataset is then split on the different attributes. The entropy for each branch is calculated.\n",
    "\n",
    "3.  Choose attribute with the largest information gain as the decision node. \n",
    "\n",
    "4a. A branch with entropy of 0 is a leaf node.\n",
    "4b. A branch with entropy more than 0 needs further splitting.\n",
    "\n",
    "5. Repeat recursive until all data is classifed or stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information Gain formula\n",
    "----\n",
    "\n",
    "IG (Y |X ) = H (Y ) − H (Y |X )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information-based Loss Function\n",
    "-----\n",
    "\n",
    "Some methods looked at uncertainty at a __single point__. \n",
    "\n",
    "What do we know about single point estimations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Single points estimations are always wrong!\n",
    "\n",
    "There must be a __better way__! Estimate distributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "￼What is Information Gain?\n",
    "------\n",
    "\n",
    "how much is the uncertainty of Y reduced, once X is known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model information gained\n",
    "------\n",
    "\n",
    "Maximize reduction in model entropy between posterior and prior (reduce number of bits required to describe distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model information gained\n",
    "------\n",
    "\n",
    "Maximize KL divergence between posterior and prior\n",
    "<center><img src=\"images/kl.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "Label each of the following:\n",
    "    Draw the diagram that connects entropy, joint entropy, conditional entropy and mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/con_entropy.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to grow a Decision Tree?\n",
    "------\n",
    "\n",
    "From top down\n",
    "\n",
    "greedily \n",
    "\n",
    "growing the tree a node at a time starting from the root\n",
    "\n",
    "When you do a split, each of the created descendant nodes corresponds to the applicable subset of the training data set. Further splits of these nodes result in new nodes that correspond to smaller subsets of data sets, and so on. Nodes that are not split further become leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When do we stop growing a  Decision Tree?\n",
    "-------\n",
    "\n",
    "Decision tree growing is done by creating a decision tree from a data set. Splits are selected, and class labels are assigned to leaves when no further splits are required or possible.\n",
    "\n",
    "The simpliest and most often used criteraion is all instances in the corresponding subset are of the same class\n",
    "The number of instances in the corresponding subset is less than a specified minimum\n",
    "The level of the current node is greater than a specified maximum, where the level of the root node is 1, and the level of its descendant nodes is 2, and so on\n",
    "The improvement of class impurity according to the best available split is less than a specified minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Decision Trees (DT) are among best the type of ML algorithms because\n",
    "    - Flexible: Discrete or continous features or targets\n",
    "    - Permonat: Great evaluation metric performance and relatively fast\n",
    "- We'll be spend most of the rest of term on them (and some of the next ML course)\n",
    "- DT decide to split based on Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- cross entropy\n",
    "- soft max classifers\n",
    "- Categorical Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
